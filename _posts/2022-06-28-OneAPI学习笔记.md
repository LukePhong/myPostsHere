---
layout:     post
title:      OneAPI学习笔记
date:       2022-06-28
author:     lukephong
header-img: img/post-bg-sycl-structure.jpg
catalog: 	 true
tags:
    - oneAPI
    - sycl
    - 并行计算
---

# OneAPI学习笔记

## by lukephong



## 我们面对的是什么东西？？

oneAPI：顾名思义，它是一套api编程接口，通过它可以控制不同类型的硬件。这个概念最大，它包含DPC++（语言）和 oneDPL oneMKL（加速库）等。编程手册[Intel® oneAPI Programming Guide](https://www.intel.com/content/www/us/en/develop/documentation/oneapi-programming-guide/top.html)(英文)

DPC++：一种可以用来给不同加速硬件编程的语言，是oneAPI的核心，可以调用oneapi的各种编程接口。包括这些部分：C++17、SYCL（可以读成C扣）、DPC++语言扩展

SYCL：是一个基于C++17的、免费的，能够用来给不同加速硬件编程的更为高级的语言（我知道这听起来很令人困惑），其实它更准确的定义是一个编程模型，也就是说我定义了一套语法规范（specification），通过这套语法能够操控各种加速硬件，但是具体是怎么操控的我根本不关心，这个看其他人自由发挥。而这里，这个人就是DPC++项目，这个项目给出了SYCL的一种实现（Implementation），也就是DPC++。

​	放一些SYCL官网上的图

<img src="https://www.khronos.org/assets/uploads/apis/2020-05-sycl-landing-page-02b_1.jpg" alt="img" style="zoom: 50%;" />

<img src="https://www.khronos.org/assets/uploads/apis/2020-05-sycl-landing-page-02a_1.jpg" alt="img" style="zoom:50%;" />

openMP:这里还要提到openMP是因为oneAPI同样支持openMP。在这里可以把omp和dpc++看成同一层次的东西。

C++17：这只是为了提醒我自己，C++标准本身包括C++标准库，也就是各种容器、算法、输入输出流等等。各种C++标准库的实现（Implementation）未必全然实现C++标准。比如CUDA非常底层，CUDA对C++标准库的实现就不包括各种容器和算法。



## 我们该怎么解决问题？

*之前提到，oneAPI给出了dpc++和omp的支持，intel显然更希望我们使用dpc++，而且至少我看过他的教程后，没有找到如何在omp下使用stl容器加GPU加速的方法。

## DPC++的使用

这里其实很大一部分是SYCL的使用，至于哪一部分是DPC++，我会单独指出。

入门三板斧：**queue**：指定任务交给谁， **buffer** ：把数据交给谁， **parallel_for**： 并行什么东西。

### 相关类型：queue\device\device selector系列\handler\\**nd_range**\\**nd_item**

```cpp
default_selector selector;
// host_selector selector;
// cpu_selector selector;
// gpu_selector selector;
```

### device类型

```c++
queue q;	//创建设备队列
device my_device = q.get_device();
//get_info()是device类型的成员函数，可以获得设备信息
std::cout << "Device: " << my_device.get_info<info::device::name>() << "\n";
```

### device selector：

用于只选择某一类的设备，如

```cpp
//只选gpu
gpu_selector selector;
queue q(selector);
```

### queue类型：

submit()函数可以将需要加速器运算的程序部分发送给这些加速器，这些程序部分被称为“命令组”（COMMAND GROUP），具体如下

```cpp
q.submit([&](handler& h) {
    //COMMAND GROUP CODE
});
```

这个↓，是一个lambda表达式，既然大家都写过java应该也都见过类似的。lambda表达式是一个函数对象，它是匿名的，从C++11之后开始被支持。

```
[]() {
  // lambda function body
};
```

[]代表着lambda表达式的开始，()用于给它传入参数，之后的{}里面就是函数体了。可以不指明它的返回类型，如果编译器可以识别出它始终返回某一特定类型的数据；也可以用[]()->type{}的写法显式指定。lambda表达式不能访问它作用域之外的变量，除非我们允许他，比如下面程序里面的num_main，正常情况下在表达式内不可见，我们通过一些方式可以让他在表达式内可见。

```c++
int num_main = 100;
// get access to num_main from the enclosing function
//传入num_main的拷贝
auto my_lambda = [num_main] () {
  cout << num_main;
};
//传入num_main的引用
auto my_lambda = [&num_main] () {
  cout << num_main;
};
//传入lambda表达式外所有变量的拷贝
auto my_lambda = [=] () {
  cout << num_main;
};
//传入lambda表达式外所有变量的引用
auto my_lambda = [&] () {
  cout << num_main;
};
```

### kernel类型：

百度翻译一下：当命令组实例化时，内核类封装了用于在设备上执行代码的方法和数据。内核对象不是由用户显式构造的，而是在调用内核调度函数（例如parallel_for）时构造的（The **kernel** class encapsulates methods and data for executing code on the device when a command group is instantiated. Kernel object is not explicitly constructed by the user and is constructed when a kernel dispatch function, such as **parallel_for**, is called）

```c++
q.submit([&](handler& h) {
    //kernel是parallel_for里面这个lambda表达式
 h.parallel_for(range<1>(N), [=](id<1> i) {
   A[i] = B[i] + C[i]);
 });
});
```

区分应用域（**Application scope**）、命令组域（**command group scope**）和核函数域（**Kernel** scope）

应用域是程序中命令组以外的地方，命令组域是命令组中核函数以外的地方。应用域和命令组域代码在host执行，能够使用完整的c++特性；核函数域在加速器上执行，只能使用部分的c++特性。

#### 基础并行核（Basic Parallel Kernels）：

基础并行核内部可以使用**range**, **id**, **item**三种不同的类的对象来控制核函数的运行。比如说以下：

```c++
//range1024步，每次走一步
//id是每个核函数的编号
h.parallel_for(range<1>(1024), [=](id<1> i){
// CODE THAT RUNS ON DEVICE 
});
//item对象代表了一个特定的kernel，包括了它是第几个，处理多大范围的数据等
h.parallel_for(range<1>(1024), [=](item<1> item){
    auto i = item.get_id();
    auto R = item.get_range();
    // CODE THAT RUNS ON DEVICE 
});
```

#### N维并行核（ND RANGE KERNELS）：

N维并行核可以和加速硬件上的计算单元（compute unit）更好的对应，并且能够利用加速器上的多级储存，从而获得更好的加速效果。整个迭代过程被划分为更小的组，称为工作组（**work-group**），一个工作组会被分配到加速器的一个运算单位上（就像一个CUDA block会被分配到一个GPU SM上）；工作组中的工作项（**work-item**）被调度在硬件上的单个计算单元上，每个工作项对应一个kernel。

N维并行核需要将nd_range和nd_item配合使用才能运行。nd_range表示了全局运行范围和本地运行范围（可以理解为work-group的范围和work-item内部的范围？类似于CUDA的gridSize和blockSize？）nd_item则和item类近似，也是能够给出当前特定kernel的信息

```c++
h.parallel_for(nd_range<1>(range<1>(1024),range<1>(64)), [=](nd_item<1> item){
    auto idx = item.get_global_id();
    auto local_id = item.get_local_id();
    // CODE THAT RUNS ON DEVICE
});
```

<img src="C:\Users\'Confidence'F\AppData\Roaming\Typora\typora-user-images\image-20220729145435678.png" alt="image-20220729145435678" style="zoom: 50%;" />

关于dpc++如何巧妙利用c++新特性实现这些功能的可以参考（[(28 封私信 / 80 条消息) 如何看待英特尔 oneAPI 编程语言 DPC++ 功能？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/504501358)）



目前，我们已经能通过kernel定义需要在device端执行的程序了，然而，device执行程序所需要的数据还无从获得。因此，就要给出缓冲区和统一共享内存两种编程模型。



## 缓冲区编程模型（Buffer Model）

简单地说就是：缓冲区（buffer）存数据，打破了host和device存储空间的界限；访问器（accessor）用来访问数据。然后程序员就不用考虑host和device之间数据移动的问题了。有两种访问器，在kernel中请使用accessor，在host代码中请使用host_accessor。关于访问器的分类、合并和访问时不能冲突的相关要求，请见sycl specification。

重复一下SYCL的几大要素：queue和selector决定在什么地方运算，buffer和accessor用于储存和访问数据，kernel是具体执行什么运算，device负责是运算发生的场所。

以下是教程中给出的典型案例，向量相加

```c++
void dpcpp_code(int* a, int* b, int* c, int N) {
  //Step 1: create a device queue创建队列
  //(developer can specify a device type via device selector or use default selector)
  auto R = range<1>(N);
  queue q;
  //Step 2: create buffers (represent both host and device memory)
    //创建缓存（可以访问host和device存储）
  buffer buf_a(a, R);
  buffer buf_b(b, R);
  buffer buf_c(c, R);
  //Step 3: submit a command for (asynchronous) execution
    //提交要执行的指令（不同次提交似乎是依次执行的，不理解括号里的asynchronous的含义）
  q.submit([&](handler &h){
      //Step 4: create buffer accessors to access buffer data on the device
          //创建accessor用于在device端访问buffer
      	//注意要指定访问的类型
      accessor A(buf_a,h,read_only);
      accessor B(buf_b,h,read_only);
      accessor C(buf_c,h,write_only);

      //Step 5: send a kernel (lambda) for execution
      //创建要执行的kernel
      h.parallel_for(range<1>(N), [=](auto i){//应该是id<1> i or item<1> i，写成auto说明编译时这里是能确定类型的
        //Step 6: write a kernel kernel的具体内容
        //Kernel invocations are executed in parallel kernel并行执行
        //Kernel is invoked for each element of the range 对range内的每一个对象kernel都执行
        //Kernel invocation has access to the invocation id kernel可以访问其id确定的位置
        C[i] = A[i] + B[i];
        });
  });
}
```

如果出现不同kernel间的数据依赖，accessor就会将这些依赖构成一张数据依赖图，然后根据图中的顺序先后执行不同的kernel。

<img src="C:\Users\'Confidence'F\AppData\Roaming\Typora\typora-user-images\image-20220729160334036.png" alt="image-20220729160334036" style="zoom: 50%;" />

(闲话：kernel里面只能用sycl::stream输出，不能用printf或cout输出，详见[Debugging - Guides - ComputeCpp CE - Products - Codeplay Developer](https://developer.codeplay.com/products/computecpp/ce/guides/sycl-guide/debugging))

对于以下代码，根据sycl specification，我认为kernel1、kernel2、kernel3应该同时执行，kernel4在1、2结束之后执行，可是其输出结果始终是以kernel定义的顺序执行，而且不论是否分布于不同的queue中，具体原因还欠调查（也许和输出的方式有关？）。

```c++
//==============================================================
// Copyright © 2020 Intel Corporation
//
// SPDX-License-Identifier: MIT
// =============================================================
#include <CL/sycl.hpp>

constexpr int num=4;
using namespace sycl;

  int main() {
  auto R = range<1>{ num };
  //Create Buffers A and B
  buffer<int> A{ R }, B{ R }, C{ R }, D{ R };
      
  //Create a device queue
  queue Q, Q1;    
  //Submit Kernel 1
  Q.submit([&](handler& h) {
    //Accessor for buffer A
    accessor out(A,h,write_only);
    sycl::stream o(1024, 256, h);
      
    h.parallel_for(R, [=](auto idx) {
      out[idx] = idx[0]; 
//         printf("1 finished!\n");
        o<<"one!"<<sycl::endl;
    }); 
  });
  //Submit Kernel 2
  Q1.submit([&](handler& h) {
    //This task will wait till the first queue is complete
    //accessor out(A,h,write_only);
      accessor out(B,h,write_only);
    sycl::stream o(1024, 256, h);
      
    h.parallel_for(R, [=](auto idx) {
      out[idx] += idx[0]; 
//         printf("2 finished!\n");
        o<<"two!"<<sycl::endl;
    });     
  });
      
  //Submit task 4
  Q.submit([&](handler& h) {
   //This task will wait till kernel 2 and 3 are complete
   accessor in (A,h,read_only);
   accessor inout(B,h);
   sycl::stream out(1024, 256, h);
      
  h.parallel_for(R, [=](auto idx) {
    inout[idx] *= in[idx]; 
//      printf("4 finished!\n");
      out<<"four!"<<sycl::endl;
  }); 
  }); 
      
  //Submit Kernel 3
  Q.submit([&](handler& h) { 
    //Accessor for Buffer B
    //accessor out(B,h,write_only);
    accessor out(C,h,write_only);
    sycl::stream o(1024, 256, h);
      
    h.parallel_for(R, [=](auto idx) {
      out[idx] = idx[0];
//          printf("3 finished!\n");
        o<<"three!"<<sycl::endl;
    }); 
  });

      
 // And the following is back to device code 这里必须是host_accessor
 host_accessor result(B,read_only);
  for (int i=0; i<num; ++i)
    std::cout << result[i] << "\n";      
  return 0;
}
```



host_accessor用于在host代码中访问buffer，它也是一种同步机制，它的实例化是一个阻塞操作，需要等待操作相关buffer的所有队列的所有kernel都结束才能完成，会将buffer数据同步回host。

要注意的是，当buffer被销毁时上述同步行为也会发生。比如下面的代码，在函数中创建了一个buffer，在函数被销毁的时候，buffer所包含的数据会被拷贝回device。

```c++
//==============================================================
// Copyright © 2020 Intel Corporation
//
// SPDX-License-Identifier: MIT
// =============================================================
#include <CL/sycl.hpp>
constexpr int N = 16;
using namespace sycl;

// Buffer creation happens within a separate function scope.
//在函数中创建buffer
void dpcpp_code(std::vector<int> &v, queue &q) {
  auto R = range<1>(N);
  buffer buf(v);
  q.submit([&](handler &h) {
    accessor a(buf,h);
    h.parallel_for(R, [=](auto i) { a[i] -= 2; });
  });
}
int main() {
  std::vector<int> v(N, 10);
  queue q;
  dpcpp_code(v, q);
  // When execution advances beyond this function scope, buffer destructor is
  // invoked which relinquishes the ownership of data and copies back the data to
  // the host memory.
  for (int i = 0; i < N; i++) std::cout << v[i] << " ";
  return 0;
}
```



## 自定义设备选择器（Custom Device Selector）

设备选择器需要给出不同设备的评分（rating），评分高的设备被优先选择。下面的代码通过继承device_selector类和重载()运算符（也就是说，选择器需要通过selector(device)的方式给出评分），给出了一个能够通过厂商名称和设备类型给出不同设备评分的自定义设备选择器。

```c++
//==============================================================
// Copyright © 2020 Intel Corporation
//
// SPDX-License-Identifier: MIT
// =============================================================
#include <CL/sycl.hpp>
#include <iostream>
using namespace sycl;
class my_device_selector : public device_selector {
public:
    my_device_selector(std::string vendorName) : vendorName_(vendorName){};
    int operator()(const device& dev) const override {
    int rating = 0;
    //We are querying for the custom device specific to a Vendor and if it is a GPU device we
    //are giving the highest rating as 3 . The second preference is given to any GPU device and the third preference is given to
    //CPU device.
    //如果是用户想要的品牌的GPU那么给最高分3分，以此类推
    if (dev.is_gpu() & (dev.get_info<info::device::name>().find(vendorName_) != std::string::npos))
        rating = 3;
    else if (dev.is_gpu()) rating = 2;
    else if (dev.is_cpu()) rating = 1;
    return rating;
    };
    
private:
    std::string vendorName_;
};
int main() {
    //pass in the name of the vendor for which the device you want to query 
    std::string vendor_name = "Intel";
    //std::string vendor_name = "AMD";
    //std::string vendor_name = "Nvidia";
    my_device_selector selector(vendor_name);
    queue q(selector);
    std::cout << "Device: "
    << q.get_device().get_info<info::device::name>() << "\n";
    return 0;
}
```



## 统一共享内存编程模型（Unified Shared Memory , USM）

统一共享内存是sycl2020的新特性，是一种基于指针的内存管理方式。程序员可以将host和device端的内存空间看成一块统一的地址空间，使用new和delete等方法进行管理，极大减轻了管理内存和迁移代码带来的负担。

统一共享内存具有显式和隐式的管理方式。这里看起来host和shared的唯一区别是shared分配空间的位置并不确定。

| Type | 函数调用 | Description | host可见 | device可见 |
|:---|:---|:---|:---:|:---:|
| Device | malloc_device | Allocation on device (显式) | NO | YES |
| Host | malloc_host |Allocation on host (隐式) | YES | YES |
| Shared | malloc_shared | Allocation can migrate between host and device (隐式) | YES | YES |

#### 分配和释放的语法

```c++
//需要给出数据类型int，个数N，队列q
int *data = malloc_shared<int>(N, q);
//这样也是可以的
int *data = static_cast<int *>(malloc_shared(N * sizeof(int), q));
//这样可以释放，q是分配时的队列
free(data, q);
```

shared方式分配的内存，会隐式的在host和device之间移动，比如下面的案例中，就可以通过直接调用kernel来使device端运行相关代码。

```c++
//==============================================================
// Copyright © 2020 Intel Corporation
//
// SPDX-License-Identifier: MIT
// =============================================================
#include <CL/sycl.hpp>
using namespace sycl;
static const int N = 16;
int main() {
  queue q;
  std::cout << "Device : " << q.get_device().get_info<info::device::name>() << "\n";

  //# USM allocation using malloc_shared
    //分配
  int *data = malloc_shared<int>(N, q);

  //# Initialize data array
    //host端初始化
  for (int i = 0; i < N; i++) data[i] = i;

  //# Modify data array on device
    //device端运算，注意要使用wait()阻塞等待device的完成
  q.parallel_for(range<1>(N), [=](id<1> i) { data[i] *= 2; }).wait();

  //# print output
    //host端输出
  for (int i = 0; i < N; i++) std::cout << data[i] << "\n";
  free(data, q);
  return 0;
}
```

当然，用户也可以通过显式的使用memcpy()函数来显式的移动数据。（尽管现在看不出是为什么）

```c++
//==============================================================
// Copyright © 2020 Intel Corporation
//
// SPDX-License-Identifier: MIT
// =============================================================
#include <CL/sycl.hpp>
using namespace sycl;
static const int N = 16;
int main() {
  queue q;
  std::cout << "Device : " << q.get_device().get_info<info::device::name>() << "\n";

  //# initialize data on host
  int *data = static_cast<int *>(malloc(N * sizeof(int)));
  for (int i = 0; i < N; i++) data[i] = i;

  //# Explicit USM allocation using malloc_device
  int *data_device = malloc_device<int>(N, q);

  //# copy mem from host to device
    //显式的向malloc_device在device分配的内存拷贝数据
  q.memcpy(data_device, data, sizeof(int) * N).wait();

  //# update device memory
    //kernel修改device端的数据
  q.parallel_for(range<1>(N), [=](id<1> i) { data_device[i] *= 2; }).wait();

  //# copy mem from device to host
    //把数据拷贝回来
  q.memcpy(data, data_device, sizeof(int) * N).wait();

  //# print output
  for (int i = 0; i < N; i++) std::cout << data[i] << "\n";
  free(data_device, q);
  free(data);
  return 0;
}
```

关于在何时使用USM，intel教程中告诉我们buffer是个好东西，但是如果你觉得buffer繁琐，就可以使用USM。

此外我们也被告知USM性能较好。但是intel的教程中说USM写程序、迁移程序方便，而那些共享分配的内存空间可能无法释放出全部性能。同时，在USM下如果需要对内存数据的移动进行控制，必须显式的分配device内存。

在[Performance Impact of USM and Buffers (intel.com)](https://www.intel.com/content/www/us/en/develop/documentation/oneapi-gpu-optimization-guide/top/memory/usm-buffer.html)一文中，intel总结了USM和Buffer使用上最基本的区别：USM可以通过指针访问数据，Buffer只能借助于相关的API（或者说accessor相当于buffer的指针）。以下判断都出自于这篇文章。

对于Buffer来说，每一个设备端的accessor创建都会把数据移动到设备端，每一次host_accessor的创建都会把数据移动回host。但是在性能上两者是存在差异的，使用accessor加下标的方式访问内存并不拖累性能，但是由于host_accessor不会移动所有数据回到host且实现上复杂，因而**host_accessor非常慢**，应该避免大规模使用。另一个值得注意的问题是，host_accessor会在创建和销毁时阻塞相关的kernel，这将花费很多时间，因而要避免如在循环中创建host_accessor等行为。

在USM中，共享内存分配在kernel运行前所有数据都会出现在device上，在这之后不会复制到host，除非它被host引用。在host端和device端访问某一块USM时，数据的移动并不是对称的。host端访问时如果内存页面不在host内存中，会触发一个系统缺页错误（operating system page fault），之后需要的页面会被从device复制到host，之后的访问如果命中已有页就正常访问，命中没有的页就再复制。但是当kernel启动时，host内存中的所有页都会被复制到device。在上述情形下，host端访问USM会因为处理缺页异常而降低性能，线性访问受到的影响较小，**跳跃访问则受较大影响**。

如果使用malloc_device在device端显式的分配内存，那么就需要像CUDA一样显式的在device和host之间进行内存拷贝。intel认为这是**具有最优性能的方式**，如果不考虑一些特定的、更细粒度的数据移动设计的话。

让我们回到oneAPI教程上。

对于同一个queue其中的任务默认按照次序“开始”而之间不相互等待（这是我理解的原文中asynchronously的意思）。这就需要用一些手段使数据冲突的kernel之间有一个执行循序。

有三种方式：

1 使用queue.wait()，这个函数阻塞host代码执行，但是阻塞到wait()之前的所有or一个kernel实行完毕还是所有和后续kernel有依赖的kernel执行完毕还是其他，这个问题还有待测试。

2 下面这样创建的queue将顺序执行所有的kernel

```c++
queue q{property::queue::in_order()};
```

3 可以在命令组中指定需要等待某一个kernel结束，如下所示

```c++
	auto e = q.submit([&](handler &h) {  // <--- e is event for kernel task
      h.parallel_for(range<1>(N), [=](id<1> i) { data[i] += 2; });
    });

    q.submit([&](handler &h) {
      h.depends_on(e);  // <--- waits until event e is complete 等待到e结束
      h.parallel_for(range<1>(N), [=](id<1> i) { data[i] += 3; });
    });
    
    //简化方案
    auto e = q.parallel_for(range<1>(N), [=](id<1> i) { data[i] += 2; }); 
    q.parallel_for(range<1>(N), e, [=](id<1> i) { data[i] += 3; });//指定要等待e，多个可使用{e1, e2}
```

可是为什么要这样做？

个人的猜测是，USM不像buffer那样需要创建accessor并且指定其类型是读是写，所以USM没法画出数据依赖图并按照它安排kernel执行，所以只能手工指出依赖关系。



## 子组（Subgroup）的使用：

什么是子组？根据intel自己的表述，在许多现代硬件平台上，工作组（work-group）中的工作项集（work-item）是同时执行的，或具有额外的调度保证。考虑已知的CUDA和SYCL对应关系，cuda block对应work-group而cuda thread对应work-item。然而一个cuda block中可以有成百上千的thread，考虑到硬件特征，这些thread都是每32个一执行，而且其动作必须完全一样，这样的32个thread被称为一个warp（本意是织布机上的经线）。这也就是上面说的“同时执行”和“额外的调度保证”了。于是乎为了更好的对应底层硬件，实现更加灵活和准确的控制以期更好的性能，所以需要有“子组”的概念。没错，在CUDA上它对应的就是warp。

以下是教程中的示例图

<img src="C:\Users\'Confidence'F\Downloads\hwmapping.png" alt="hwmapping" style="zoom: 80%;" />

为了让大家看的清楚，把这张展示了执行单元（EU）内部结构的图也放出来，同时给出一张nv volta架构的图

<img src="C:\Users\'Confidence'F\Downloads\GPU_Arch.png" alt="GPU_Arch" style="zoom:67%;" />

<img src="https://pic1.zhimg.com/80/v2-7e8aec0cd1982065fa3a7f40cec1e184_1440w.jpg" alt="img" style="zoom:67%;" />

如果把intel的subslice和nv的sm看作是同一个层次的东西的话，那么可以看到intel一个subslice里面至少分了8个EU，然后每个EU里面有同步执行的SIMD通道，他们是要干一样的指令的，然后一个EU对应的就是一个子组（subgroup），每个子组里面的一个工作项（work-item）对应一条SIMD通道。再看nv这边，每一个SM内部也被分成了4个组，这个分组方式是从maxwell架构以后才出现的，以前只有两个组。可以看到这一个组内部的结构很明显是比intel的EU要复杂得多的。但是由于这一个组里只有一个warp scheduler，这里的运算单元只能同步执行相同的指令。对应的，SM中的这样一个组对应一个子组，里面的每一条运算通道（目前是32条），对用一个工作项。

上面这一段是我个人的理解，可能有一定的问题，欢迎大家批评指正。

### 子组的基本用法

subgroup handle是对子组进行操作的基础，可以这样来创建它，这会发生在ND kernel内部。

```c++
sycl::sub_group sg = nd_item.get_sub_group(); //也可用auto类型
```

获取子组的信息：

```c++
	h.parallel_for(nd_range<1>(64,64), [=](nd_item<1> item){
      /* get sub_group handle */
      auto sg = item.get_sub_group();
      /* query sub_group and print sub_group info once per sub_group */
        //get_local_id()返回当前work-item在子组里面的index
      if(sg.get_local_id()[0] == 0){
        out << "sub_group id: " << sg.get_group_id()[0]	//返回当前子组的index
            << " of " << sg.get_group_range()[0]	//返回work-group里面有多少subgroup
            << ", size=" << sg.get_local_range()[0] 	//返回子组大小
            << "\n";
      }
    });
```

子组的大小可能需要自己设置，虽然编译器也会在没设置时自动指定，但是也可以在加速器支持的范围内选择，比如intel的GPU支持8、16、32这样的设定。可以通过下面的方式查看支持的子组大小值。

```c++
auto sg_sizes = q.get_device().get_info<info::device::sub_group_sizes>();
```

使用下面的方式指定子组大小：

```c++
q.parallel_for(nd_range<1>(N, B), [=](nd_item<1> item)[[intel::reqd_sub_group_size(16)]] {                         
        // Kernel Code
}).wait();
```

根据C++11标准，[[]]里面的是一个属性说明符序列（Attribute specifier sequence）。reqd_sub_group_size()中的数值必须在编译时能够确定。

### 好用的方法和算法

这些算法能够实现实际上很大程度上依赖于子组的特性，或者说硬件特性。

shuffle方法：

由于子组内部的工作项之间可以直接通信，这样可以减少每个kernel本地内存的使用和/或访问全局内存（显存）的次数。

下面的图片展示了data[i] = permute_group_by_xor(sg, data[i], 1);的运行效果，permute的意思就是交换，只需要把每个index的二进制写下来然后跟1做异或，你就知道为什么是和旁边的数据做交换了。

<img src="C:\Users\'Confidence'F\AppData\Roaming\Typora\typora-user-images\image-20220730230455065.png" alt="image-20220730230455065" style="zoom: 50%;" />

此外还有下面这些相关函数。
- select_by_group(sg, x, id)
- `shift_group_left(sg, x, delta)`
- `shift_group_right(sg, x, delta)`
- `permute_group_by_xor(sg, x, mask)`

reduce方法：

我们通过一个函数就可以实现子组的规约操作，包括求和、求极值等。

```c++
h.parallel_for(nd_range<1>(N,B), [=](nd_item<1> item){
      auto sg = item.get_sub_group();
      auto i = item.get_global_id(0);
      /* Reduction algorithm on Sub-group */
      int result = reduce_over_group(sg, data[i], plus<>());
      //int result = reduce_over_group(sg, data[i], maximum<>());
      //int result = reduce_over_group(sg, data[i], minimum<>());
    });
```

广播方法：

可以让子组内的所有kernel获得某一kernel的值，展示如下：

```c++
//3是要被广播的值的index
data[i] = group_broadcast(sg, data[i], 3);
```

投票方法：

描述如下

```c++
	//input位置实际上是这个kernel的投票值，应该理解为一个bool值；
	//结果也是bool值，即“所有kernel投1本组结果都为1”等，以此类推
	all[i] = all_of_group(sg, input[i]);
    any[i] = any_of_group(sg, input[i]);
    none[i] = none_of_group(sg, input[i]);
```



## oneAPI DPC++ Library ***(oneDPL)***的容器、算法和使用

这才是真正的dpc++的东西，是intel的贡献，也是SYCL所不涵盖的地方。

oneDPL包括一系列可供并行执行的STL容器、算法和一些拓展，涵盖了我们熟悉的绝大多数c++标准中的内容。

比如你可以用以下方式，在GPU上为vector排序：

```c++
sycl::queue q(sycl::gpu_selector{});
std::sort(oneapi::dpl::execution::make_device_policy(q), v.begin(), v.end());
```

或使用特定的值填充一个vector

```c++
//该算法是使用Buffer的方式实现的
std::fill(oneapi::dpl::execution::make_device_policy(q), v.begin(), v.end(), 20);
```

### 使用的三种方式

概要：

1对于支持的C++标准API，可以通过直接include其头文件并使用namespace std即可使用。支持的API列表可以在[Tested Standard C++ APIs — oneAPI Libraries Documentation 1.0 documentation](https://docs.oneapi.io/versions/latest/onedpl/tested_standard_cpp_api.html?highlight=handler)找到。

2对于**Parallel STL**，它是C++17标准的一部分，使用方法可以参考[Get Started with Parallel STL (intel.com)](https://www.intel.com/content/www/us/en/developer/articles/guide/get-started-with-parallel-stl.html)。

3对于DPL拓展的API，使用方法可以在[Extension API — oneAPI Libraries Documentation 1.0 documentation](https://docs.oneapi.io/versions/latest/onedpl/extension_api.html)找到。

1、3需要的头文件都可以通过#include <oneapi/dpl/文件名>包含。对于1，如果直接包含如#include <utility>就需要使用namespace std，如果#include<oneapi/dpl/utility>，就需要使用namespace oneapi::dpl。2的使用需要安装其他依赖，这里就不展开了。

方式一：直接使用

可以像下面一样直接、连续的使用DPL算法。这种方式通过Buffer实现，会创建临时的Buffer，在算法执行前后，数据会会拷贝入临时的buffer中，之后再拷贝回来。

```c++
//==============================================================
// Copyright © 2020 Intel Corporation
//
// SPDX-License-Identifier: MIT
// =============================================================
//注意包含的头文件
#include <oneapi/dpl/algorithm>
#include <oneapi/dpl/execution>
#include<CL/sycl.hpp>
using namespace sycl;
using namespace oneapi::dpl::execution;

int main() {
  queue q;
  std::cout << "Device : " << q.get_device().get_info<info::device::name>() << "\n";
  std::vector<int> v{2,3,1,4};
    
  std::for_each(make_device_policy(q), v.begin(), v.end(), [](int &a){ a *= 2; });
  std::sort(make_device_policy(q), v.begin(), v.end());
    
  for(int i = 0; i < v.size(); i++) std::cout << v[i] << "\n";
  return 0;
}
```

方法二：通过Buffer调用

很明显，如果要连续使用DPL算法且操作的是同一块数据，那么像直接使用那样在device和host之间来回拷贝数据就没有意义了。Buffer则可以使需要处理的数据一直停留在device上，直到buffer销毁才拷贝回来。注意，为了让数据能够回来，我们必须把buffer销毁，也就是说，距离buffer实例化处最近的那一对{}是不能省略的。

需要更多包含一个头文件#include <oneapi/dpl/iterator>

然后你不需要写任何lambda表达式，直接传入buffer然后在代码块中调用相关算法即可。

```c++
//==============================================================
// Copyright © 2020 Intel Corporation
//
// SPDX-License-Identifier: MIT
// =============================================================

#include <oneapi/dpl/algorithm>
#include <oneapi/dpl/execution>
#include <oneapi/dpl/iterator>
#include <CL/sycl.hpp>
using namespace sycl;
using namespace oneapi::dpl::execution;
int main(){
  queue q;
  std::cout << "Device : " << q.get_device().get_info<info::device::name>() << "\n";
  std::vector<int> v{2,3,1,4};
    
  //# Create a buffer and use buffer iterators in Parallel STL algorithms
    //先用vector容器创建buffer。然后获得buffer开始和结束的指针（？）
  {
    buffer buf(v);
    auto buf_begin = oneapi::dpl::begin(buf);
    auto buf_end   = oneapi::dpl::end(buf);

    std::for_each(make_device_policy(q), buf_begin, buf_end, [](int &a){ a *= 3; });
    std::sort(make_device_policy(q), buf_begin, buf_end);
  }
    
  for(int i = 0; i < v.size(); i++) std::cout << v[i] << "\n";
  return 0;
}
```

方法三：USM法

通过USM使用DPL有两种方式：通过指针和通过allocators。

先介绍通过指针的方式。通过下面的例子相信能够很快的掌握其应用方法。

```c++
//==============================================================
// Copyright © 2020 Intel Corporation
//
// SPDX-License-Identifier: MIT
// =============================================================
//注意需要include的头文件
#include <oneapi/dpl/algorithm>
#include <oneapi/dpl/execution>
using namespace sycl;
using namespace oneapi::dpl::execution;
const int N = 4;

int main() {
  queue q;
  std::cout << "Device : " << q.get_device().get_info<info::device::name>() << "\n";
    
  //# USM allocation on device 使用正常的方式分配usm内存
  int* data = malloc_shared<int>(N, q);
    
  //# Parallel STL algorithm using USM pointer 直接调用所需函数，data+N显然是指向结尾的指针
  std::fill(make_device_policy(q), data, data + N, 20);
  q.wait();//注意需要等待执行结束
    
  for (int i = 0; i < N; i++) std::cout << data[i] << "\n";
  free(data, q);
  return 0;
}
```

上面分配的空间是一个数组，可是如果想要使用STL容器的话就只能使用Allocator了。

在这种情况下我们必须先创建allocator，然后用它来创建对应的vector，具体展示如下。经过测试，一个allocator可以用于在一个设备上分配不同的多个容器。

```c++
#include <oneapi/dpl/algorithm>
#include <oneapi/dpl/execution>

using namespace sycl;
using namespace oneapi::dpl::execution;

const int N = 4;

int main() {
  queue q;
  std::cout << "Device : " << q.get_device().get_info<info::device::name>() << "\n";
    
  //# USM allocator 创建usm_allocator，使用共享内存模式
  usm_allocator<int, usm::alloc::shared> alloc(q);
  //用usm_allocator创建vector
  std::vector<int, decltype(alloc)> v(N, alloc), v1(N, alloc);
    
  //# Parallel STL algorithm with USM allocator
  std::fill(make_device_policy(q), v.begin(), v.end(), 20);
  q.wait();
  std::fill(make_device_policy(q), v1.begin(), v1.end(), 30);
  q.wait();
    
  for (int i = 0; i < v.size(); i++) std::cout << v[i] << "\n";
  for (int i = 0; i < v1.size(); i++) std::cout << v1[i] << "\n";
  return 0;
}
```

