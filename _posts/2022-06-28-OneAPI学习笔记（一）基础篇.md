---
layout:     post
title:      2022-06-28-OneAPI学习笔记（一）基础篇
date:       2022-06-28
author:     lukephong
header-img: img/post-bg-sycl-structure.jpg
catalog: 	 true
tags:
    - oneAPI
    - sycl
    - 并行计算
---

# OneAPI学习笔记（一）基础篇

by lukephong


## 我们面对的是什么东西？？

oneAPI：顾名思义，它是一套api编程接口，通过它可以控制不同类型的硬件。这个概念最大，它包含DPC++（语言）和 oneDPL oneMKL（加速库）等。编程手册[Intel® oneAPI Programming Guide](https://www.intel.com/content/www/us/en/develop/documentation/oneapi-programming-guide/top.html)(英文)

DPC++：一种可以用来给不同加速硬件编程的语言，是oneAPI的核心，可以调用oneapi的各种编程接口。包括这些部分：C++17、SYCL（可以读成C扣）、DPC++语言扩展

SYCL：是一个基于C++17的、免费的，能够用来给不同加速硬件编程的更为高级的语言（我知道这听起来很令人困惑），其实它更准确的定义是一个编程模型，也就是说我定义了一套语法规范（specification），通过这套语法能够操控各种加速硬件，但是具体是怎么操控的我根本不关心，这个看其他人自由发挥。而这里，这个人就是DPC++项目，这个项目给出了SYCL的一种实现（Implementation），也就是DPC++。

​	放一些SYCL官网上的图

<img src="https://www.khronos.org/assets/uploads/apis/2020-05-sycl-landing-page-02b_1.jpg" alt="img" style="zoom: 50%;" />

<img src="https://www.khronos.org/assets/uploads/apis/2020-05-sycl-landing-page-02a_1.jpg" alt="img" style="zoom:50%;" />

openMP:这里还要提到openMP是因为oneAPI同样支持openMP。在这里可以把omp和dpc++看成同一层次的东西。

C++17：这只是为了提醒我自己，C++标准本身包括C++标准库，也就是各种容器、算法、输入输出流等等。各种C++标准库的实现（Implementation）未必全然实现C++标准。比如CUDA非常底层，CUDA对C++标准库的实现就不包括各种容器和算法。



## 我们该怎么解决问题？

*之前提到，oneAPI给出了dpc++和omp的支持，intel显然更希望我们使用dpc++，而且至少我看过他的教程后，没有找到如何在omp下使用stl容器+GPU加速的方法。

## DPC++的使用

这里其实很大一部分是SYCL的使用，至于哪一部分是DPC++，我会单独指出。

入门三板斧：**queue**：指定任务交给谁， **buffer** ：把数据交给谁， **parallel_for**： 并行什么东西。

### 相关类型：queue\device\device selector系列\handler\\**nd_range**\\**nd_item**

```cpp
default_selector selector;
// host_selector selector;
// cpu_selector selector;
// gpu_selector selector;
```

### device类型

```c++
queue q;	//创建设备队列
device my_device = q.get_device();
//get_info()是device类型的成员函数，可以获得设备信息
std::cout << "Device: " << my_device.get_info<info::device::name>() << "\n";
```

### device selector：

用于只选择某一类的设备，如

```cpp
//只选gpu
gpu_selector selector;
queue q(selector);
```

### queue类型：

submit()函数可以将需要加速器运算的程序部分发送给这些加速器，这些程序部分被称为“命令组”（COMMAND GROUP），具体如下

```cpp
q.submit([&](handler& h) {
    //COMMAND GROUP CODE
});
```

这个↓，是一个lambda表达式，既然大家都写过java应该也都见过类似的。lambda表达式是一个函数对象，它是匿名的，从C++11之后开始被支持。

```
[]() {
  // lambda function body
};
```

[]代表着lambda表达式的开始，()用于给它传入参数，之后的{}里面就是函数体了。可以不指明它的返回类型，如果编译器可以识别出它始终返回某一特定类型的数据；也可以用[]()->type{}的写法显式指定。lambda表达式不能访问它作用域之外的变量，除非我们允许他，比如下面程序里面的num_main，正常情况下在表达式内不可见，我们通过一些方式可以让他在表达式内可见。

```c++
int num_main = 100;
// get access to num_main from the enclosing function
//传入num_main的拷贝
auto my_lambda = [num_main] () {
  cout << num_main;
};
//传入num_main的引用
auto my_lambda = [&num_main] () {
  cout << num_main;
};
//传入lambda表达式外所有变量的拷贝
auto my_lambda = [=] () {
  cout << num_main;
};
//传入lambda表达式外所有变量的引用
auto my_lambda = [&] () {
  cout << num_main;
};
```

### kernel类型：

百度翻译一下：当命令组实例化时，内核类封装了用于在设备上执行代码的方法和数据。内核对象不是由用户显式构造的，而是在调用内核调度函数（例如parallel_for）时构造的（The **kernel** class encapsulates methods and data for executing code on the device when a command group is instantiated. Kernel object is not explicitly constructed by the user and is constructed when a kernel dispatch function, such as **parallel_for**, is called）

```c++
q.submit([&](handler& h) {
    //kernel是parallel_for里面这个lambda表达式
 h.parallel_for(range<1>(N), [=](id<1> i) {
   A[i] = B[i] + C[i]);
 });
});
```

区分应用域（**Application scope**）、命令组域（**command group scope**）和核函数域（**Kernel** scope）

应用域是程序中命令组以外的地方，命令组域是命令组中核函数以外的地方。应用域和命令组域代码在host执行，能够使用完整的c++特性；核函数域在加速器上执行，只能使用部分的c++特性。

#### 基础并行核（Basic Parallel Kernels）：

基础并行核内部可以使用**range**, **id**, **item**三种不同的类的对象来控制核函数的运行。比如说以下：

```c++
//range1024步，每次走一步
//id是每个核函数的编号
h.parallel_for(range<1>(1024), [=](id<1> i){
// CODE THAT RUNS ON DEVICE 
});
//item对象代表了一个特定的kernel，包括了它是第几个，处理多大范围的数据等
h.parallel_for(range<1>(1024), [=](item<1> item){
    auto i = item.get_id();
    auto R = item.get_range();
    // CODE THAT RUNS ON DEVICE 
});
```

#### N维并行核（ND RANGE KERNELS）：

N维并行核可以和加速硬件上的计算单元（compute unit）更好的对应，并且能够利用加速器上的多级储存，从而获得更好的加速效果。整个迭代过程被划分为更小的组，称为工作组（**work-group**），一个工作组会被分配到加速器的一个运算单位上（就像一个CUDA block会被分配到一个GPU SM上）；工作组中的工作项（**work-item**）被调度在硬件上的单个计算单元上，每个工作项对应一个kernel。

N维并行核需要将nd_range和nd_item配合使用才能运行。nd_range表示了全局运行范围和本地运行范围（可以理解为work-group的范围和work-item内部的范围？类似于CUDA的gridSize和blockSize？）nd_item则和item类近似，也是能够给出当前特定kernel的信息

```c++
h.parallel_for(nd_range<1>(range<1>(1024),range<1>(64)), [=](nd_item<1> item){
    auto idx = item.get_global_id();
    auto local_id = item.get_local_id();
    // CODE THAT RUNS ON DEVICE
});
```

<img src="https://lukephong.github.io/BlogImages//image-20220729145435678.png" alt="image-20220729145435678" style="zoom: 50%;" />

关于dpc++如何巧妙利用c++新特性实现这些功能的可以参考（[(28 封私信 / 80 条消息) 如何看待英特尔 oneAPI 编程语言 DPC++ 功能？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/504501358)）



目前，我们已经能通过kernel定义需要在device端执行的程序了，然而，device执行程序所需要的数据还无从获得。因此，就要给出缓冲区和统一共享内存两种编程模型。

