---
layout:     post
title:      2022-06-29-OneAPI学习笔记（二）编程模型
date:       2022-06-29
author:     lukephong
header-img: img/post-bg-sycl-structure.jpg
catalog: 	 true
tags:
    - oneAPI
    - sycl
    - 并行计算
---

# OneAPI学习笔记（二）编程模型

by lukephong

## 缓冲区编程模型（Buffer Model）

简单地说就是：缓冲区（buffer）存数据，打破了host和device存储空间的界限；访问器（accessor）用来访问数据。然后程序员就不用考虑host和device之间数据移动的问题了。有两种访问器，在kernel中请使用accessor，在host代码中请使用host_accessor。关于访问器的分类、合并和访问时不能冲突的相关要求，请见sycl specification。

重复一下SYCL的几大要素：queue和selector决定在什么地方运算，buffer和accessor用于储存和访问数据，kernel是具体执行什么运算，device负责是运算发生的场所。

以下是教程中给出的典型案例，向量相加

```c++
void dpcpp_code(int* a, int* b, int* c, int N) {
  //Step 1: create a device queue创建队列
  //(developer can specify a device type via device selector or use default selector)
  auto R = range<1>(N);
  queue q;
  //Step 2: create buffers (represent both host and device memory)
    //创建缓存（可以访问host和device存储）
  buffer buf_a(a, R);
  buffer buf_b(b, R);
  buffer buf_c(c, R);
  //Step 3: submit a command for (asynchronous) execution
    //提交要执行的指令（不同次提交似乎是依次执行的，不理解括号里的asynchronous的含义）
  q.submit([&](handler &h){
      //Step 4: create buffer accessors to access buffer data on the device
          //创建accessor用于在device端访问buffer
      	//注意要指定访问的类型
      accessor A(buf_a,h,read_only);
      accessor B(buf_b,h,read_only);
      accessor C(buf_c,h,write_only);

      //Step 5: send a kernel (lambda) for execution
      //创建要执行的kernel
      h.parallel_for(range<1>(N), [=](auto i){//应该是id<1> i or item<1> i，写成auto说明编译时这里是能确定类型的
        //Step 6: write a kernel kernel的具体内容
        //Kernel invocations are executed in parallel kernel并行执行
        //Kernel is invoked for each element of the range 对range内的每一个对象kernel都执行
        //Kernel invocation has access to the invocation id kernel可以访问其id确定的位置
        C[i] = A[i] + B[i];
        });
  });
}
```

如果出现不同kernel间的数据依赖，accessor就会将这些依赖构成一张数据依赖图，然后根据图中的顺序先后执行不同的kernel。

<img src="https://lukephong.github.io/BlogImages//image-20220729160334036.png.png" alt="image-20220729160334036" style="zoom: 50%;" />

(闲话：kernel里面只能用sycl::stream输出，不能用printf或cout输出，详见[Debugging - Guides - ComputeCpp CE - Products - Codeplay Developer](https://developer.codeplay.com/products/computecpp/ce/guides/sycl-guide/debugging))

对于以下代码，根据sycl specification，我认为kernel1、kernel2、kernel3应该同时执行，kernel4在1、2结束之后执行，可是其输出结果始终是以kernel定义的顺序执行，而且不论是否分布于不同的queue中，具体原因还欠调查（也许和输出的方式有关？）。

```c++
//==============================================================
// Copyright © 2020 Intel Corporation
//
// SPDX-License-Identifier: MIT
// =============================================================
#include <CL/sycl.hpp>

constexpr int num=4;
using namespace sycl;

  int main() {
  auto R = range<1>{ num };
  //Create Buffers A and B
  buffer<int> A{ R }, B{ R }, C{ R }, D{ R };
      
  //Create a device queue
  queue Q, Q1;    
  //Submit Kernel 1
  Q.submit([&](handler& h) {
    //Accessor for buffer A
    accessor out(A,h,write_only);
    sycl::stream o(1024, 256, h);
      
    h.parallel_for(R, [=](auto idx) {
      out[idx] = idx[0]; 
//         printf("1 finished!\n");
        o<<"one!"<<sycl::endl;
    }); 
  });
  //Submit Kernel 2
  Q1.submit([&](handler& h) {
    //This task will wait till the first queue is complete
    //accessor out(A,h,write_only);
      accessor out(B,h,write_only);
    sycl::stream o(1024, 256, h);
      
    h.parallel_for(R, [=](auto idx) {
      out[idx] += idx[0]; 
//         printf("2 finished!\n");
        o<<"two!"<<sycl::endl;
    });     
  });
      
  //Submit task 4
  Q.submit([&](handler& h) {
   //This task will wait till kernel 2 and 3 are complete
   accessor in (A,h,read_only);
   accessor inout(B,h);
   sycl::stream out(1024, 256, h);
      
  h.parallel_for(R, [=](auto idx) {
    inout[idx] *= in[idx]; 
//      printf("4 finished!\n");
      out<<"four!"<<sycl::endl;
  }); 
  }); 
      
  //Submit Kernel 3
  Q.submit([&](handler& h) { 
    //Accessor for Buffer B
    //accessor out(B,h,write_only);
    accessor out(C,h,write_only);
    sycl::stream o(1024, 256, h);
      
    h.parallel_for(R, [=](auto idx) {
      out[idx] = idx[0];
//          printf("3 finished!\n");
        o<<"three!"<<sycl::endl;
    }); 
  });

      
 // And the following is back to device code 这里必须是host_accessor
 host_accessor result(B,read_only);
  for (int i=0; i<num; ++i)
    std::cout << result[i] << "\n";      
  return 0;
}
```



host_accessor用于在host代码中访问buffer，它也是一种同步机制，它的实例化是一个阻塞操作，需要等待操作相关buffer的所有队列的所有kernel都结束才能完成，会将buffer数据同步回host。

要注意的是，当buffer被销毁时上述同步行为也会发生。比如下面的代码，在函数中创建了一个buffer，在函数被销毁的时候，buffer所包含的数据会被拷贝回device。

```c++
//==============================================================
// Copyright © 2020 Intel Corporation
//
// SPDX-License-Identifier: MIT
// =============================================================
#include <CL/sycl.hpp>
constexpr int N = 16;
using namespace sycl;

// Buffer creation happens within a separate function scope.
//在函数中创建buffer
void dpcpp_code(std::vector<int> &v, queue &q) {
  auto R = range<1>(N);
  buffer buf(v);
  q.submit([&](handler &h) {
    accessor a(buf,h);
    h.parallel_for(R, [=](auto i) { a[i] -= 2; });
  });
}
int main() {
  std::vector<int> v(N, 10);
  queue q;
  dpcpp_code(v, q);
  // When execution advances beyond this function scope, buffer destructor is
  // invoked which relinquishes the ownership of data and copies back the data to
  // the host memory.
  for (int i = 0; i < N; i++) std::cout << v[i] << " ";
  return 0;
}
```



## 自定义设备选择器（Custom Device Selector）

设备选择器需要给出不同设备的评分（rating），评分高的设备被优先选择。下面的代码通过继承device_selector类和重载()运算符（也就是说，选择器需要通过selector(device)的方式给出评分），给出了一个能够通过厂商名称和设备类型给出不同设备评分的自定义设备选择器。

```c++
//==============================================================
// Copyright © 2020 Intel Corporation
//
// SPDX-License-Identifier: MIT
// =============================================================
#include <CL/sycl.hpp>
#include <iostream>
using namespace sycl;
class my_device_selector : public device_selector {
public:
    my_device_selector(std::string vendorName) : vendorName_(vendorName){};
    int operator()(const device& dev) const override {
    int rating = 0;
    //We are querying for the custom device specific to a Vendor and if it is a GPU device we
    //are giving the highest rating as 3 . The second preference is given to any GPU device and the third preference is given to
    //CPU device.
    //如果是用户想要的品牌的GPU那么给最高分3分，以此类推
    if (dev.is_gpu() & (dev.get_info<info::device::name>().find(vendorName_) != std::string::npos))
        rating = 3;
    else if (dev.is_gpu()) rating = 2;
    else if (dev.is_cpu()) rating = 1;
    return rating;
    };
    
private:
    std::string vendorName_;
};
int main() {
    //pass in the name of the vendor for which the device you want to query 
    std::string vendor_name = "Intel";
    //std::string vendor_name = "AMD";
    //std::string vendor_name = "Nvidia";
    my_device_selector selector(vendor_name);
    queue q(selector);
    std::cout << "Device: "
    << q.get_device().get_info<info::device::name>() << "\n";
    return 0;
}
```



## 统一共享内存编程模型（Unified Shared Memory , USM）

统一共享内存是sycl2020的新特性，是一种基于指针的内存管理方式。程序员可以将host和device端的内存空间看成一块统一的地址空间，使用new和delete等方法进行管理，极大减轻了管理内存和迁移代码带来的负担。

统一共享内存具有显式和隐式的管理方式。这里看起来host和shared的唯一区别是shared分配空间的位置并不确定。

| Type | 函数调用 | Description | host可见 | device可见 |
|:---|:---|:---|:---:|:---:|
| Device | malloc_device | Allocation on device (显式) | NO | YES |
| Host | malloc_host |Allocation on host (隐式) | YES | YES |
| Shared | malloc_shared | Allocation can migrate between host and device (隐式) | YES | YES |

#### 分配和释放的语法

```c++
//需要给出数据类型int，个数N，队列q
int *data = malloc_shared<int>(N, q);
//这样也是可以的
int *data = static_cast<int *>(malloc_shared(N * sizeof(int), q));
//这样可以释放，q是分配时的队列
free(data, q);
```

shared方式分配的内存，会隐式的在host和device之间移动，比如下面的案例中，就可以通过直接调用kernel来使device端运行相关代码。

```c++
//==============================================================
// Copyright © 2020 Intel Corporation
//
// SPDX-License-Identifier: MIT
// =============================================================
#include <CL/sycl.hpp>
using namespace sycl;
static const int N = 16;
int main() {
  queue q;
  std::cout << "Device : " << q.get_device().get_info<info::device::name>() << "\n";

  //# USM allocation using malloc_shared
    //分配
  int *data = malloc_shared<int>(N, q);

  //# Initialize data array
    //host端初始化
  for (int i = 0; i < N; i++) data[i] = i;

  //# Modify data array on device
    //device端运算，注意要使用wait()阻塞等待device的完成
  q.parallel_for(range<1>(N), [=](id<1> i) { data[i] *= 2; }).wait();

  //# print output
    //host端输出
  for (int i = 0; i < N; i++) std::cout << data[i] << "\n";
  free(data, q);
  return 0;
}
```

当然，用户也可以通过显式的使用memcpy()函数来显式的移动数据。（尽管现在看不出是为什么）

```c++
//==============================================================
// Copyright © 2020 Intel Corporation
//
// SPDX-License-Identifier: MIT
// =============================================================
#include <CL/sycl.hpp>
using namespace sycl;
static const int N = 16;
int main() {
  queue q;
  std::cout << "Device : " << q.get_device().get_info<info::device::name>() << "\n";

  //# initialize data on host
  int *data = static_cast<int *>(malloc(N * sizeof(int)));
  for (int i = 0; i < N; i++) data[i] = i;

  //# Explicit USM allocation using malloc_device
  int *data_device = malloc_device<int>(N, q);

  //# copy mem from host to device
    //显式的向malloc_device在device分配的内存拷贝数据
  q.memcpy(data_device, data, sizeof(int) * N).wait();

  //# update device memory
    //kernel修改device端的数据
  q.parallel_for(range<1>(N), [=](id<1> i) { data_device[i] *= 2; }).wait();

  //# copy mem from device to host
    //把数据拷贝回来
  q.memcpy(data, data_device, sizeof(int) * N).wait();

  //# print output
  for (int i = 0; i < N; i++) std::cout << data[i] << "\n";
  free(data_device, q);
  free(data);
  return 0;
}
```

关于在何时使用USM，intel教程中告诉我们buffer是个好东西，但是如果你觉得buffer繁琐，就可以使用USM。

此外我们也被告知USM性能较好。但是intel的教程中说USM写程序、迁移程序方便，而那些共享分配的内存空间可能无法释放出全部性能。同时，在USM下如果需要对内存数据的移动进行控制，必须显式的分配device内存。

在[Performance Impact of USM and Buffers (intel.com)](https://www.intel.com/content/www/us/en/develop/documentation/oneapi-gpu-optimization-guide/top/memory/usm-buffer.html)一文中，intel总结了USM和Buffer使用上最基本的区别：USM可以通过指针访问数据，Buffer只能借助于相关的API（或者说accessor相当于buffer的指针）。以下判断都出自于这篇文章。

对于Buffer来说，每一个设备端的accessor创建都会把数据移动到设备端，每一次host_accessor的创建都会把数据移动回host。但是在性能上两者是存在差异的，使用accessor加下标的方式访问内存并不拖累性能，但是由于host_accessor不会移动所有数据回到host且实现上复杂，因而**host_accessor非常慢**，应该避免大规模使用。另一个值得注意的问题是，host_accessor会在创建和销毁时阻塞相关的kernel，这将花费很多时间，因而要避免如在循环中创建host_accessor等行为。

在USM中，共享内存分配在kernel运行前所有数据都会出现在device上，在这之后不会复制到host，除非它被host引用。在host端和device端访问某一块USM时，数据的移动并不是对称的。host端访问时如果内存页面不在host内存中，会触发一个系统缺页错误（operating system page fault），之后需要的页面会被从device复制到host，之后的访问如果命中已有页就正常访问，命中没有的页就再复制。但是当kernel启动时，host内存中的所有页都会被复制到device。在上述情形下，host端访问USM会因为处理缺页异常而降低性能，线性访问受到的影响较小，**跳跃访问则受较大影响**。

如果使用malloc_device在device端显式的分配内存，那么就需要像CUDA一样显式的在device和host之间进行内存拷贝。intel认为这是**具有最优性能的方式**，如果不考虑一些特定的、更细粒度的数据移动设计的话。

让我们回到oneAPI教程上。

对于同一个queue其中的任务默认按照次序“开始”而之间不相互等待（这是我理解的原文中asynchronously的意思）。这就需要用一些手段使数据冲突的kernel之间有一个执行循序。

有三种方式：

1 使用queue.wait()，这个函数阻塞host代码执行，但是阻塞到wait()之前的所有or一个kernel实行完毕还是所有和后续kernel有依赖的kernel执行完毕还是其他，这个问题还有待测试。

2 下面这样创建的queue将顺序执行所有的kernel

```c++
queue q{property::queue::in_order()};
```

3 可以在命令组中指定需要等待某一个kernel结束，如下所示

```c++
	auto e = q.submit([&](handler &h) {  // <--- e is event for kernel task
      h.parallel_for(range<1>(N), [=](id<1> i) { data[i] += 2; });
    });

    q.submit([&](handler &h) {
      h.depends_on(e);  // <--- waits until event e is complete 等待到e结束
      h.parallel_for(range<1>(N), [=](id<1> i) { data[i] += 3; });
    });
    
    //简化方案
    auto e = q.parallel_for(range<1>(N), [=](id<1> i) { data[i] += 2; }); 
    q.parallel_for(range<1>(N), e, [=](id<1> i) { data[i] += 3; });//指定要等待e，多个可使用{e1, e2}
```

可是为什么要这样做？

个人的猜测是，USM不像buffer那样需要创建accessor并且指定其类型是读是写，所以USM没法画出数据依赖图并按照它安排kernel执行，所以只能手工指出依赖关系。



## 子组（Subgroup）的使用：

什么是子组？根据intel自己的表述，在许多现代硬件平台上，工作组（work-group）中的工作项集（work-item）是同时执行的，或具有额外的调度保证。考虑已知的CUDA和SYCL对应关系，cuda block对应work-group而cuda thread对应work-item。然而一个cuda block中可以有成百上千的thread，考虑到硬件特征，这些thread都是每32个一执行，而且其动作必须完全一样，这样的32个thread被称为一个warp（本意是织布机上的经线）。这也就是上面说的“同时执行”和“额外的调度保证”了。于是乎为了更好的对应底层硬件，实现更加灵活和准确的控制以期更好的性能，所以需要有“子组”的概念。没错，在CUDA上它对应的就是warp。

以下是教程中的示例图

<img src="https://lukephong.github.io/BlogImages//hwmapping.png.png" alt="hwmapping" style="zoom: 80%;" />

为了让大家看的清楚，把这张展示了执行单元（EU）内部结构的图也放出来，同时给出一张nv volta架构的图

<img src="https://lukephong.github.io/BlogImages//GPU_Arch.png" alt="GPU_Arch" style="zoom:67%;" />

<img src="https://pic1.zhimg.com/80/v2-7e8aec0cd1982065fa3a7f40cec1e184_1440w.jpg" alt="img" style="zoom:67%;" />

如果把intel的subslice和nv的sm看作是同一个层次的东西的话，那么可以看到intel一个subslice里面至少分了8个EU，然后每个EU里面有同步执行的SIMD通道，他们是要干一样的指令的，然后一个EU对应的就是一个子组（subgroup），每个子组里面的一个工作项（work-item）对应一条SIMD通道。再看nv这边，每一个SM内部也被分成了4个组，这个分组方式是从maxwell架构以后才出现的，以前只有两个组。可以看到这一个组内部的结构很明显是比intel的EU要复杂得多的。但是由于这一个组里只有一个warp scheduler，这里的运算单元只能同步执行相同的指令。对应的，SM中的这样一个组对应一个子组，里面的每一条运算通道（目前是32条），对用一个工作项。

上面这一段是我个人的理解，可能有一定的问题，欢迎大家批评指正。

### 子组的基本用法

subgroup handle是对子组进行操作的基础，可以这样来创建它，这会发生在ND kernel内部。

```c++
sycl::sub_group sg = nd_item.get_sub_group(); //也可用auto类型
```

获取子组的信息：

```c++
	h.parallel_for(nd_range<1>(64,64), [=](nd_item<1> item){
      /* get sub_group handle */
      auto sg = item.get_sub_group();
      /* query sub_group and print sub_group info once per sub_group */
        //get_local_id()返回当前work-item在子组里面的index
      if(sg.get_local_id()[0] == 0){
        out << "sub_group id: " << sg.get_group_id()[0]	//返回当前子组的index
            << " of " << sg.get_group_range()[0]	//返回work-group里面有多少subgroup
            << ", size=" << sg.get_local_range()[0] 	//返回子组大小
            << "\n";
      }
    });
```

子组的大小可能需要自己设置，虽然编译器也会在没设置时自动指定，但是也可以在加速器支持的范围内选择，比如intel的GPU支持8、16、32这样的设定。可以通过下面的方式查看支持的子组大小值。

```c++
auto sg_sizes = q.get_device().get_info<info::device::sub_group_sizes>();
```

使用下面的方式指定子组大小：

```c++
q.parallel_for(nd_range<1>(N, B), [=](nd_item<1> item)[[intel::reqd_sub_group_size(16)]] {                         
        // Kernel Code
}).wait();
```

根据C++11标准，[[]]里面的是一个属性说明符序列（Attribute specifier sequence）。reqd_sub_group_size()中的数值必须在编译时能够确定。

### 好用的方法和算法

这些算法能够实现实际上很大程度上依赖于子组的特性，或者说硬件特性。

shuffle方法：

由于子组内部的工作项之间可以直接通信，这样可以减少每个kernel本地内存的使用和/或访问全局内存（显存）的次数。

下面的图片展示了data[i] = permute_group_by_xor(sg, data[i], 1);的运行效果，permute的意思就是交换，只需要把每个index的二进制写下来然后跟1做异或，你就知道为什么是和旁边的数据做交换了。

<img src="https://lukephong.github.io/BlogImages//image-20220730230455065.png.png" alt="image-20220730230455065" style="zoom: 50%;" />

此外还有下面这些相关函数。
- select_by_group(sg, x, id)
- `shift_group_left(sg, x, delta)`
- `shift_group_right(sg, x, delta)`
- `permute_group_by_xor(sg, x, mask)`

reduce方法：

我们通过一个函数就可以实现子组的规约操作，包括求和、求极值等。

```c++
h.parallel_for(nd_range<1>(N,B), [=](nd_item<1> item){
      auto sg = item.get_sub_group();
      auto i = item.get_global_id(0);
      /* Reduction algorithm on Sub-group */
      int result = reduce_over_group(sg, data[i], plus<>());
      //int result = reduce_over_group(sg, data[i], maximum<>());
      //int result = reduce_over_group(sg, data[i], minimum<>());
    });
```

广播方法：

可以让子组内的所有kernel获得某一kernel的值，展示如下：

```c++
//3是要被广播的值的index
data[i] = group_broadcast(sg, data[i], 3);
```

投票方法：

描述如下

```c++
	//input位置实际上是这个kernel的投票值，应该理解为一个bool值；
	//结果也是bool值，即“所有kernel投1本组结果都为1”等，以此类推
	all[i] = all_of_group(sg, input[i]);
    any[i] = any_of_group(sg, input[i]);
    none[i] = none_of_group(sg, input[i]);
```

